<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2023-06-15">

<title>The State of AI in Drug Development ‚Äì Data Woman</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../favicon.svg" rel="icon" type="image/svg+xml">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Data Woman</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-data-for-science" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Data for Science</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-data-for-science">    
        <li>
    <a class="dropdown-item" href="../pages/ether0.html">
 <span class="dropdown-text">Science AI in 2025</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../pages/ai-earlyeve.html">
 <span class="dropdown-text">Science AI in 2023</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../pages/the-data-stupid.html">
 <span class="dropdown-text">It‚Äôs the data, stupid</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-making-orgs-work" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Making Orgs Work</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-making-orgs-work">    
        <li>
    <a class="dropdown-item" href="../pages/posit-keynote.html">
 <span class="dropdown-text">Starting up data science</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../pages/metrics-reality.html">
 <span class="dropdown-text">What metrics are for</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../pages/agile-for-ds.html">
 <span class="dropdown-text">Agile for data science</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../pages/pain-of-ds.html">
 <span class="dropdown-text">The pain of DS in orgs</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-tech-tools" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Tech Tools</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-tech-tools">    
        <li>
    <a class="dropdown-item" href="../pages/serverless-apps.html">
 <span class="dropdown-text">Serverless data apps</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../pages/vibe-coding.html">
 <span class="dropdown-text">Why vibe coding</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../pages/methods-chatbot.html">
 <span class="dropdown-text">Chatbots</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-women-in-leadership" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Women in Leadership</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-women-in-leadership">    
        <li>
    <a class="dropdown-item" href="../pages/jane-goodall.html">
 <span class="dropdown-text">Jane Goodall</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../pages/onlyness.html">
 <span class="dropdown-text">The loneliness of onlyness</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../pages/asterisks.html">
 <span class="dropdown-text">Jacinda Ardern</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../pages/schario.html">
 <span class="dropdown-text">Agile at home</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-the-nature-of-ai" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">The nature of AI</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-the-nature-of-ai">    
        <li>
    <a class="dropdown-item" href="../pages/bitter-lesson.html">
 <span class="dropdown-text">The bitter lesson</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../pages/how-llms-work.html">
 <span class="dropdown-text">What is an AI model, really?</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-magical-ingredients" id="toc-the-magical-ingredients" class="nav-link active" data-scroll-target="#the-magical-ingredients">The Magical Ingredients</a></li>
  <li><a href="#fertile-ground-for-ai" id="toc-fertile-ground-for-ai" class="nav-link" data-scroll-target="#fertile-ground-for-ai">Fertile Ground for AI?</a>
  <ul class="collapse">
  <li><a href="#protein-folding" id="toc-protein-folding" class="nav-link" data-scroll-target="#protein-folding">Protein Folding</a></li>
  </ul></li>
  <li><a href="#whats-next" id="toc-whats-next" class="nav-link" data-scroll-target="#whats-next">What‚Äôs Next?</a>
  <ul class="collapse">
  <li><a href="#the-bull-case" id="toc-the-bull-case" class="nav-link" data-scroll-target="#the-bull-case">The Bull Case</a></li>
  <li><a href="#the-bear-case" id="toc-the-bear-case" class="nav-link" data-scroll-target="#the-bear-case">The Bear Case</a></li>
  <li><a href="#my-take" id="toc-my-take" class="nav-link" data-scroll-target="#my-take">My Take</a></li>
  </ul></li>
  <li><a href="#how-are-companies-using-ai-today" id="toc-how-are-companies-using-ai-today" class="nav-link" data-scroll-target="#how-are-companies-using-ai-today">How are companies using AI today?</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The State of AI in Drug Development</h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">June 15, 2023</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><em>Note from the future (2025): this is an article I wrote for the founding team as EvE was just coming into being on the state of AI and its implications for drug development. Consider it a time capsule back to 2023 ‚Äì ages ago in the AI world!</em></p>
<section id="the-magical-ingredients" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-magical-ingredients">The Magical Ingredients</h2>
<p>The notable broadly recognized progress of AI-type capabilities in the last few years is the result of three necessary ingredients coming together to produce seemingly magical leaps forward in language, image, and code generation among other things.</p>
<p>Methods + Compute + Data = MagicüîÆ.</p>
<section id="methods" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="methods">Methods</h4>
<p>Neural networks are the class of methods powering modern advances, but they have been around since the 1940s. Modeled on the human brain, neural networks have layers that begin as a naive structure, but learn from training data what the essential patterns are in an arbitrarily complex way that can function as a black box.</p>
<p>After multiple cycles of favor and disfavor (mostly caused by computational limitations), neural networks emerged in the 2010s in the form of ‚Äúdeep learning‚Äù, which began to consistently demonstrate superior performance on a diverse set of tasks. Deep learning is a term for a neural network with more than three layers.</p>
<p>There are many types of neural networks. Some only move in one direction (‚Äúfeed forward‚Äù), some circle back through layers (‚Äúrecurrent‚Äù), some pool local information (‚Äúconvolutional‚Äù), and so forth. Different types are suited to different tasks. The application of neural networks to any particular problem requires a series of judgment calls about how to design and train the system.</p>
<p>The application of deep learning in the early 2010s resulted in meaningful advances in image processing tasks, but not in language. The design of language processing approaches was increasingly complex, attempting to represent what humans knew about language in the architecture. Performance was underwhelming.</p>
<div class="page-columns page-full"><p>Then in the mid-2010s, a team at Google Brain working on language translation discovered the benefits of a different architecture and published the seminal paper <em>Attention Is All You Need (2017)</em>. Known as transformers, this approach backed off on the representation of complexity and instead allowed the network more flexibility in learning patterns itself.  A critical feature of this architecture is that it is easily parallelized, leading to the ability to process much larger amounts of data. This essentially represented a tradeoff in which the model was responsible for more of the ‚Äúintelligence‚Äù, but was given much more data from which to learn. This ended up being a very good tradeoff, leading to the impressive language models of the early 2020s.</p><div class="no-row-height column-margin column-container"><span class="margin-aside"><a href="https://daleonai.com/transformers-explained">This 2021 blog post from Dale Markowitz</a> is a very accessible high-level introduction to transformers. It describes the key elements of transformers without getting into anything highly technical, and summarizes the broad arc of their impact.</span></div></div>
<p>What turns out to be magical about the transformer architecture is that it excels at many tasks well beyond language translation. At the moment it is the golden ticket to progress that has not yet run out, and most of the AI developments since 2021 involve transformers. Expertise is still required to determine how best to apply them to new areas, but for the time being they seem to be broadly applicable across domains. No doubt at some unknown point in the future progress will stall, awaiting another advance in methods.</p>
</section>
<section id="compute" class="level4">
<h4 class="anchored" data-anchor-id="compute">Compute</h4>
<p>The success of the massive data + massive parallelization approach was only possible due to the availability of massive compute capability, specifically graphical processing units (GPUs). The initial rise of GPUs was for gaming. In the 2010s, they began to be used for deep learning, and then experienced additional demand for crypto mining. The recent AI hype and its demand for GPUs made GPU supplier NVIDIA a trillion dollar company in May 2023.</p>
<p>We can assume that compute capability is domain-agnostic. It is expensive, however, so the business model of a given endeavor will determine willingness to pay.</p>
</section>
<section id="data" class="level4">
<h4 class="anchored" data-anchor-id="data">Data</h4>
<p>The raw material required to power the AI machine is data. <em>Massive amounts</em> of data. The most prominent AI models were trained on data harvested from publicly available sources ‚Äì writing on the internet, digitized books, digitized art, and code posted to Github. Humanity had already produced the data, and the internet made it machine-ready. Recent learning has demonstrated that the sheer volume of data is critical, with iterative improvements on models largely correlating with the increasing size of training datasets.</p>
<p>Many of the new models fall into a category dubbed ‚Äúfoundation models‚Äù. Instead of being trained for a specific task, they are allowed to learn from data in a very general way that produces broadly applicable capabilities. These foundation models can then be used as a base from which specific models can be trained using small, specific datasets (a process known as ‚Äúfine-tuning‚Äù). Large language models (LLMs) like GPT are an example of a foundation model. In this sense, datasets are being repurposed across applications to some degree. Regardless, data is by far the most domain-specific of the three ingredients.</p>
<p>So, in assessing the AI-readiness and potential in various arenas, we should consider the availability of large scale high quality data first, methods second, and (willingness to pay for) compute third.</p>
</section>
</section>
<section id="fertile-ground-for-ai" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="fertile-ground-for-ai">Fertile Ground for AI?</h2>
<p>Does the hypothesis that methods, compute, and data are the requirements for AI success hold for biology and drug development? There is some positive evidence.</p>
<section id="protein-folding" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="protein-folding">Protein Folding</h3>
<div class="page-columns page-full"><p>The most prominent AI triumph in the scientific realm is the AlphaFold protein folding model. The original AlphaFold model from Google DeepMind performed well, but <a href="https://www.nature.com/articles/s41586-021-03819-2_">AlphaFold2 (2021)</a> ‚Äì in which the transformer architecture was applied ‚Äì did substantially better. Notably, it out-competed other transformer-based approaches, in a testament to the importance of expertise in applying methods to specific domains and problems. </p><div class="no-row-height column-margin column-container"><span class="margin-aside"><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10011655/">This 2022 Frontier in Bioinformatics article</a> reviews the developments in protein folding models, along with the strengths and weaknesses of the various approaches.</span></div></div>
<div class="page-columns page-full"><p>The key ingredient of data was available for the protein folding application due to the vast numbers of sequenced proteins, of which a non-trivial portion have solved structures available. AlphaFold was able to take advantage of both this ‚Äúlabeled‚Äù data on structures and learn patterns from the sequences without structures. This represents an important difference between the current AI approaches and more traditional applications of machine learning. Traditionally, this type of prediction model would being limited to the ~150,000 proteins with available structures, which would then need to be split for use in both training and validation processes.  The ability of AI systems to learn effectively from unlabeled data (through ‚Äúunsupervised‚Äù and ‚Äúself-supervised‚Äù processes), provides ways to make use of data without being entirely bottlenecked by expensive or impossible labeling processes.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">A layperson‚Äôs description of how AlphaFold2 works can be found <a href="https://daleonai.com/how-alphafold-works">in this blog post</a>.</span></div></div>
<p>Quickly following after AlphaFold were another type of protein folding models based on large language model approaches. These models rely only on the protein sequence itself (no multi-sequence alignment), and treat amino acids like words and proteins like sentences. ESMFold (from Facebook Research) is an example of this type of model. While less accurate than AlphaFold2, it is much less computationally intensive, and thus can be applied on a metagenomic scale.</p>
<p>These models have provided an abundance of structures that are readily available and are in fruitful use by researchers. AlphaFold2 alone has taken the structural coverage for human proteins from 48% to 76%.</p>
<section id="a-few-other-key-ingredients" class="level4">
<h4 class="anchored" data-anchor-id="a-few-other-key-ingredients">A few other key ingredients</h4>
<p>In addition to the big three ingredients outlined previously, also important for AlphaFold2‚Äôs success were:</p>
<ul>
<li>The right cross-disciplinary team in the right working environment</li>
<li>A problem with strong patterns to learn</li>
</ul>
<p>It‚Äôs not clear where critical domain-specific AI developments will happen in the future. The ability to attract, assemble, and support the necessary team has seemed possible only under the wing of major technology companies to date. As AI becomes more mainstream, that may change, but academia and pharma/techbio companies both have some substantial challenges in this regard.</p>
<p>Thanks to evolution and the nature of protein folding itself (with secondary and tertiary structures), the type of patterns AI systems are good at learning are present in the protein folding problem. Not all problems will be as well suited, and consequently may require even larger amounts of data and/or novel or hybrid methods.</p>
</section>
<section id="limitations-and-root-causes" class="level4">
<h4 class="anchored" data-anchor-id="limitations-and-root-causes">Limitations and root causes</h4>
<p>Protein folding models are AI success stories, but have weaknesses of particular relevance to drug development. For example, AlphaFold2 has trouble predicting longer loops, which are important because of their surface exposure. It also struggles to predict structures with ligands, DNA/RNA complexes or post-translational modifications. Many of these challenges go back to the nature of the training data itself. Structures sourced from the Protein Data Bank (PDB) can represent many contexts and conditions, and these are not accounted for in the model itself. Other challenges may be due to the complexity of the problem. AlphaFold2 does well at predicting shorter loops, with under 20 residues. Longer, more flexible loops and unstructured regions of proteins generally are inherently harder to predict.</p>
</section>
</section>
</section>
<section id="whats-next" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="whats-next">What‚Äôs Next?</h2>
<p>Protein folding shows us that AI progress can carry over into biological applications. But does that mean we should expect a near-term rapid acceleration of progress in drug development and therapeutics as a result of AI?</p>
<section id="the-bull-case" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-bull-case">The Bull Case</h3>
<div class="page-columns page-full"><p>The bull case for AI is that the biological world is an excellent fit for the nature of AI, and that we‚Äôve just reached the point where data generation at the necessary scale is possible.  DeepMind CEO Demis Hassabis has argued that biology can be seen as a complex information processing system, making it ripe to be decoded by AI systems that can theoretically learn patterns at a scale that humans cannot.</p><div class="no-row-height column-margin column-container"><span class="margin-aside"><a href="https://centuryofbio.com/p/physical-predictions">This Centry of Biology blog post</a> lays out the big picture (and VC-style optimistic) case for why biology is a great AI target, much better than other applications in the physical world.</span></div></div>
<div class="page-columns page-full"><p>The nature of data in the biological realm is that it can‚Äôt be scraped from existing sources as training data for LLMs has been from the internet. It requires instruments, measurement systems, and physical substrates. However, there is optimism that <em>now is the time</em> that we are ready to generate the type of data that‚Äôs needed, at least in several key areas.  The ability to sequence, synthesize, and edit DNA at reasonable cost and speed is enabling data generation on a new scale. Whole genome sequencing for millions of people is within reach, potentially creating the type of massive datasets necessary for AI success.</p><div class="no-row-height column-margin column-container"><span class="margin-aside">One optimistic voice is Daphne Koller, CEO of Insitro and well-respected AI researcher. She believes a few publicly available datasets like the UK Biobank are fit for ML use, but that most data will need to be specifically generated. <a href="https://www.mckinsey.com/industries/life-sciences/our-insights/it-will-be-a-paradigm-shift-daphne-koller-on-machine-learning-in-drug-discovery">This McKinsey article</a> has a brief summary of her perspective. A broader, richer conversation is in <a href="https://bio-eats-world.simplecast.com/episodes/ai-and-actionable-insights">this Bio Eats World podcast episode.</a> Another such voice is Jakob Uszkoreit, an author of the Google Brain transformers paper, now working on applications of AI to RNA as CEO at Inceptive. He discusses his perspective in <a href="https://a16z.com/2023/01/11/bio-eats-world-using-ai-to-take-bio-farther/">this Bio Eats World podcast episode</a>.</span></div></div>
<p>One path to success could be through the compounding effects of a growing number of specific AI models. As we have models that are very good at a particular task (like AlphaFold is for protein structure prediction), these could then be connected in a modular way to become more than the sum of their parts. This could help us tackle the unfathomable complexity of the biological world one piece at a time, while getting the benefits of chaining capabilities together in a way that can still result in a fully machine-driven system.</p>
<p>Another path could be to amass huge amounts of heterogeneous data and impose less structure representing processes as we see them. This tradeoff has served us well recently, and we have generally underestimated the complexity of patterns AI can learn. Perhaps viewing protein folding as a modular capability is not as good as allowing a system to learn from observations that reflect the context in which proteins operate. It‚Äôs possible that large numbers of whole genome sequences paired with digital medical records could power such a system. In the UK, where WGS efforts have been most extensive to date (UK Biobank) <em>and</em> the NHS has extensive medical records in a unified system, the ability to attempt this may be within reach.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="margin-aside"><a href="https://www.nature.com/articles/s42256-022-00463-x">This Nature article</a> reviews the many steps of the drug discovery process on which GPU computing and deep learning capabilities are having a positive impact.</span></div></div>
<p>Additionally, advances in lab automation capabilities may unlock the ability to generate and use data on a fundamentally different scale. A virtuous cycle of data collection that feeds back into the models and accelerates their performance could be possible (and some companies are attempting this approach today). These ‚Äúclosed-loop‚Äù systems could operate at a speed and scale inconceivable when human scientists must always be in the loop, leading to an inflection point in progress.</p>
<p>Even if the progress driven by AI is more incremental than exponential for some time, it has the potential to improve performance at many steps of the drug development process. In an industry with long cycles and high failure rates, AI could lead to substantial business gains even before it becomes truly transformative.</p>
</section>
<section id="the-bear-case" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="the-bear-case">The Bear Case</h3>
<p>The bear case for AI is that the biological world is orders of magnitude more complex than anything AI has been successfully applied to to date, and our ability to collect data on it is highly imperfect and expensive. The extent of data collection required would be beyond the capacity of any given entity, and the incentives don‚Äôt promise sufficient payoff in a single area to justify even a consortium of players making such a tremendous investment. While more advances akin to those we‚Äôve seen in protein folding are likely for specific cases, and will drive incremental progress, the breadth of progress required to see transformational impact at the level of human health may require many more step change advances in AI capabilities that would likely take decades.</p>
<div class="page-columns page-full"><p>Hints that the current wave of progress could solve some hard problems in biology but leave the many <em>very</em> hard problems untouched come again from protein folding models. Despite the capabilities of AlphaFold, we are unable to model what we might care most about for drug development - binding with compounds.  One can argue that compounds are much less amenable to wrangling by current AI systems because they:</p><div class="no-row-height column-margin column-container"><span class="margin-aside"><a href="https://www.science.org/content/blog-post/give-me-those-hard-hard-numbers">In this 2023 article</a> Derek Lowe makes the argument that ligand-binding predictions are a much harder problem than protein folding predictions, and that we lack the necessary data.</span></div></div>
<ul>
<li>have more complex constructions than sequences of nucleic or amino acids</li>
<li>have more possible individual building blocks</li>
<li>have no single convenient representation akin to an amino acid sequence</li>
<li>lack information from what exists in the biological world that constrains the total potential space of possibilities to a more tractable subset</li>
</ul>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="margin-aside"><a href="https://centuryofbio.com/p/machine-brains-and-their-discontents">This 2023 Century of Biology blog post</a> explores the arenas of tangible success and failure-to-date of AI systems in biological applications. The hypothesis put forth is that we‚Äôre showing substantial progress in <strong>modality</strong> companies, but not in <strong>target discovery</strong> endeavors, because the ripeness of the problems for AI is fundamentally different.</span></div></div>
<p>While the hypothetical number of protein sequences is 20^(sequence length of your choosing), the universe of proteins that exist in the world is much smaller, providing at least an initial fruitful constraint on the problem space. The possible chemical space is comparatively enormous. Furthermore, the specification of the problem for drugs is less clear than for protein folding. Predicting a protein structure from an amino acid sequence is a well specified problem for which there are ‚Äúright‚Äù answers available. Identifying drug candidates is a more complex problem that has to be broken down into components. One step could be identifying compounds that bind to a particular target. This is well specified and can be validated experimentally. However, this is one of many steps necessary to produce an effective drug. The data needed to validate the therapeutic value of drugs is extremely slow and expensive to collect in clinical trials, making the feedback loop quite challenging.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="margin-aside"><a href="https://www.americanpharmaceuticalreview.com/Featured-Articles/597596-Exploring-New-Chemical-Space-for-the-Treatments-of-Tomorrow/">This article by Verseon CEO Adityo Prakash (not an unbiased voice)</a> speculates about the difficulties and possibilities of applying AI and computational methods to the chemical space.</span></div></div>
<p>So, compounds in chemical space may be a very hard problem. Target discovery is also likely to be a hard problem, as it requires modeling more complexity than AI systems have succeeded at to date. Problems like protein folding are certainly challenging, but have the specificity of task and clear measurement of success that AI systems thrive on. Target discovery does not.</p>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="margin-aside"><a href="https://www.mckinsey.com/industries/life-sciences/our-insights/better-data-for-better-therapies-the-case-for-building-health-data-platforms">This 2022 McKinsey article</a> explores the promise and challenges of health data platforms.</span></div></div>
<p>Then there is the very, very hard problem of clinical trials. If progress in identifying potential therapeutics rapidly accelerates, it will still be bottlenecked by the speed and massive expense of clinical trials (which currently make up ~70% of R&amp;D costs). Unless the odds of success in clinical studies increases, it may exceed our ability to move candidates through this critical step.</p>
<p>Another challenge is management of medical records data in a way that it can be a useful input for AI learning. Reams of potentially valuable data exist in digital format, but the barriers to making them accessible are substantial.</p>
</section>
<section id="my-take" class="level3">
<h3 class="anchored" data-anchor-id="my-take">My Take</h3>
<p>If I were forced to make predictions‚Ä¶</p>
<section id="next-five-years" class="level4">
<h4 class="anchored" data-anchor-id="next-five-years">Next Five Years</h4>
<ul>
<li>Progress in end-to-end drug discovery will be incremental rather than transformational.
<ul>
<li>Success rates in the clinic of ‚ÄúAI drugs‚Äù will be similar to the standard process, but will get to the clinical phase more quickly and with more novel compounds and disease areas than pharma companies have seen. This will fall short of the hype, but will be enough to motivate continued partnerships with and investment in AI-centric companies.</li>
<li><em>But</em> if the current set of drugs in clinical trials has a lower success rate than the existing process, we‚Äôre likely to spend at least five years in the ‚Äútrough of disillusionment‚Äù, with greatly reduced investment. It may then take a major development from a tech company to motivate another round of attempts.</li>
</ul></li>
<li>We‚Äôll see transformational progress outside the area of small molecule drugs, based on genomics and/or biologics. These are the areas that seem most well-suited for major AI impacts using the existing methods and formula.</li>
</ul>
</section>
<section id="ten-years-out" class="level4">
<h4 class="anchored" data-anchor-id="ten-years-out">Ten Years Out</h4>
<ul>
<li>‚ÄúClosed loop‚Äù drug development systems will start to bear fruit, based on the level of scale that automated experimentation has enabled. The impact of these will depend on how much confidence they can generate that their compounds will have high success in the clinic.</li>
<li>A combination of methodological and computing innovations will make exploration of chemical space more tractable, leading to the ability to design truly novel drugs.</li>
<li>Models will emerge that take genomics inputs and predict disease states and therapeutic responsiveness, largely ignoring the mechanisms in between. Their progress will be based on the use of AI to process medical records data into an anonymized and usable form across systems, which may require enabling legal/regulatory changes.</li>
</ul>
<p>If R&amp;D becomes more efficient with a new set of approaches, what will established pharma companies do?</p>
<ul>
<li>Continue to partner with techbio companies?</li>
<li>Try to bring this capability in-house? (which I would expect to have a high rate of failure due to incompatible cultures and ways of operating)</li>
<li>Get out-competed by new companies who increasingly have the ability to bring the drugs they discover to market on their own?</li>
</ul>
<div style="page-break-after: always;"></div>
</section>
</section>
</section>
<section id="how-are-companies-using-ai-today" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="how-are-companies-using-ai-today">How are companies using AI today?</h2>
<div class="page-columns page-full"><p>A number of companies have been founded in the last decade with the goal of applying ML/AI to drug development. Several of these now have compounds in clinical trials, but it‚Äôs rather early to assess their success. No ‚ÄúAI-designed‚Äù drug has yet achieved FDA approval. </p><div class="no-row-height column-margin column-container"><span class="margin-aside"><a href="https://www.nature.com/articles/s41591-023-02361-0?utm_source=substack&amp;utm_medium=email">This Nature article</a> reflects on the state of this industry and summaries the current clinical status of compounds from this set of companies.</span></div></div>
<p>Most of these companies seem to operate in a general pattern involving:</p>
<ul>
<li>A ‚Äúfull-stack‚Äù / ‚Äúclosed-loop‚Äù approach in which they develop (or acquire) multiple capabilities with the promise of a virtuous cycle of data generation that accelerates a flywheel of progress</li>
<li>A proprietary software/data/algorithms platform that integrates their components and adds their AI/ML/compute secret sauce</li>
<li>Partnerships with pharmaceutical companies</li>
</ul>
<p>The nature of the IP portfolios varies more widely with some companies (ex. BenevolentAI) appearing to have a more classic pharma focus on compounds and specific therapeutic areas, and others with ML/AI components and computing/automation methods related to the ‚Äúclosed loop‚Äù capabilities. Generally, these companies seem to acknowledge the importance of generating large amounts of fit-for-ML proprietary datasets, and of being able to integrate several steps of the drug discovery process rather than focusing on mastering a single area. So, a fundamentally different type of effort from DeepMind‚Äôs AlphaFold work.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "Óßã";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/datawoman\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>